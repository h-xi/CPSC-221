So far: 

When building a Dictionary we are able to achieve: 

Unsorted Arrays: (insert, O(1)), (find, O(n)), (delete, O(n))

Balanced Trees: (insert, O(logn)), (find, O(logn)), (delete, O(logn))

We want to achieve... O(1) for ALL! 


What are the issues? 

- Using a resizable vector --> O(1)?
    - Insert --> inserting at very large index --> bad space complexity - exponential
        - Circular Array --> modulo arithmetic 
            - Collision --> Potentially overwriting existing data eg. index 37 == index 5
                - Double size everytime we collide 
                - Punt to another Dictionary --> Every index is a unsorted linked list 
                    - Measure how full --> load factor 
                    - Copying everything over --> Retry based on new size 
                    - load factor = # of keys/array size --> when load factor gets too big --> resize 
                - Punt to another slot in existing dictionary --> probing 
                    - Modest load factor 
            - Hashing --> Turn whatever type key and convert into an integer within bounded range 
                - Good hash function: 
                    - Easy (fast) to compute O(1)
                    - Distributes the keys uniformly across the table 
                    - Deterministic 

Hashing Collisions: 

Hash Function General Principals: 

- Use the entire search key in the hash functin 
- If the hash function uses modulo arithmetic make the table size a prime number 
- A simple and usually effective hash function is 
    - convert the key value to an integer x 
    - h(x) = x mod tablesize 
        - where tablesize is the first prime number larger than twice the size of the number of expected values 

Open Addressing: 

- When insertion results in collision look for an empty array element 
    - Start at index to which the hash function mapped the inserted item 
    - Look for a free space in the array following a particular search pattern (probing)

    Open Addressing Schemes: 

    - Linear probing --> simple 
        - For each time table is probed for free location add one to index 
        - If sequence of probes reaches last element, wrap around to arr[0]
        - Primary clustering: 
            - Groups of consecutively occupied locations 
            - Get larger as time goes on --> reducing hash table efficiency 
    - Double hashing -> key dependant probe sequences
        - Second hash function determines probe sequences 
        - Second hash function must follow: 
            - h2(key) != 0 
            - h2 != h1 
            - Typical h2 is p - (key mod p) where p is a prime # 
            - First probe, go # of h2 steps, second probe + h2 again etc. 

Separate Chaining: 

- Each entry in hash table is pointer to linked list 
    - If collision occurs new item is added to end of list at appropriate location 
- Performance degrades less rapidly using separate chaining 
    - With uniform random distribution, separate chaining mantains a good performance even at high load factors greater than 
    

Efficiency: 

- As load factor increases, chance of collision increases 
    - Performance decreases as load factor increases 
- Unsuccessful search makes more comparisons --> Only ends when a free element is found 
- Table size should be selected such that load factor does not exceed 1/2 

Removals: 

- Mark a table location as either empty, occupied or removed 
    - Locations in the removed state can be re-sed as items are inserted 
        - After confirming non-existence 
- After many removes, table may become clogged with tombstones 
    - Forms similar cluster to linear probing 
    - Periodically re-hashing all valid items will solve this issue. 

IF load factor is less than 1/2 BOTH open addressing and separate chaining give similar performances
    - As lambda increases, separate chaining performs better 
    - Separate chaining increases storage overhead for linked list pointers 

- In the WORST case, hash table performance can be very bad 
    - Hash function does not unformily distirbute data across the table



